{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34882ab3-8c2d-4ade-a619-9772c2e8c0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 09:49:23.077247: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-15 09:49:23.116150: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/jaumemanero/RL/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2025-12-15 09:49:24.053705: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a8d6c44-45e2-43fa-819b-6884032cd88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(details.get(\"device_name\", \"Unknown GPU\"))\n",
    "else:\n",
    "    print(\"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71851b55-b5dc-4e18-b878-a894e5e6cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490ce283-6959-478f-b537-1556f2814cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total questions: 50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CharacterTable:\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = \"0123456789+ \"\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print(\"Generating data...\")\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(\n",
    "        \"\".join(\n",
    "            np.random.choice(list(\"0123456789\"))\n",
    "            for i in range(np.random.randint(1, DIGITS + 1))\n",
    "        )\n",
    "    )\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = \"{}+{}\".format(a, b)\n",
    "    query = q + \" \" * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print(\"Total questions:\", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ceb842-9b4d-4e8d-86de-8a971d44a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorization...\")\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Validation Data:\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f71f2e-ca75-4da4-a5ac-6f137574e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765788592.034778  120037 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21770 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:52:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">72,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,548</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m72,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m12\u001b[0m)          │         \u001b[38;5;34m1,548\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,324</span> (802.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m205,324\u001b[0m (802.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,324</span> (802.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m205,324\u001b[0m (802.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Build model...\")\n",
    "num_layers = 1  # Try to add more LSTM layers!\n",
    "\n",
    "model = keras.Sequential()\n",
    "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(layers.Input((MAXLEN, len(chars))))\n",
    "model.add(layers.LSTM(128))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(num_layers):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd2f2b0f-1ea1-4068-8fab-9e8d28a2c57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 09:50:00.088525: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 36ms/step - accuracy: 0.3622 - loss: 1.7438 - val_accuracy: 0.4291 - val_loss: 1.5422\n",
      "Q 721+2   T 723  \u001b[91m☒ 222 \u001b[0m\n",
      "Q 18+719  T 737  \u001b[91m☒ 225 \u001b[0m\n",
      "Q 672+16  T 688  \u001b[91m☒ 762 \u001b[0m\n",
      "Q 18+202  T 220  \u001b[91m☒ 225 \u001b[0m\n",
      "Q 180+37  T 217  \u001b[91m☒ 395 \u001b[0m\n",
      "Q 52+392  T 444  \u001b[91m☒ 354 \u001b[0m\n",
      "Q 97+301  T 398  \u001b[91m☒ 314 \u001b[0m\n",
      "Q 131+6   T 137  \u001b[91m☒ 222 \u001b[0m\n",
      "Q 3+548   T 551  \u001b[91m☒ 444 \u001b[0m\n",
      "Q 294+49  T 343  \u001b[91m☒ 495 \u001b[0m\n",
      "\n",
      "Iteration 2\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 34ms/step - accuracy: 0.5127 - loss: 1.3015 - val_accuracy: 0.5849 - val_loss: 1.1226\n",
      "Q 3+534   T 537  \u001b[91m☒ 547 \u001b[0m\n",
      "Q 584+63  T 647  \u001b[91m☒ 638 \u001b[0m\n",
      "Q 47+64   T 111  \u001b[91m☒ 112 \u001b[0m\n",
      "Q 897+714 T 1611 \u001b[91m☒ 1618\u001b[0m\n",
      "Q 786+35  T 821  \u001b[91m☒ 822 \u001b[0m\n",
      "Q 28+557  T 585  \u001b[91m☒ 592 \u001b[0m\n",
      "Q 56+16   T 72   \u001b[91m☒ 66  \u001b[0m\n",
      "Q 241+99  T 340  \u001b[91m☒ 322 \u001b[0m\n",
      "Q 180+29  T 209  \u001b[91m☒ 292 \u001b[0m\n",
      "Q 150+12  T 162  \u001b[92m☑ 162 \u001b[0m\n",
      "\n",
      "Iteration 3\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 42ms/step - accuracy: 0.6195 - loss: 1.0229 - val_accuracy: 0.6568 - val_loss: 0.9427\n",
      "Q 287+486 T 773  \u001b[91m☒ 851 \u001b[0m\n",
      "Q 695+81  T 776  \u001b[91m☒ 771 \u001b[0m\n",
      "Q 9+343   T 352  \u001b[91m☒ 351 \u001b[0m\n",
      "Q 12+978  T 990  \u001b[91m☒ 988 \u001b[0m\n",
      "Q 415+538 T 953  \u001b[91m☒ 964 \u001b[0m\n",
      "Q 647+468 T 1115 \u001b[91m☒ 1118\u001b[0m\n",
      "Q 59+89   T 148  \u001b[91m☒ 141 \u001b[0m\n",
      "Q 267+57  T 324  \u001b[92m☑ 324 \u001b[0m\n",
      "Q 641+81  T 722  \u001b[91m☒ 721 \u001b[0m\n",
      "Q 69+904  T 973  \u001b[91m☒ 978 \u001b[0m\n",
      "\n",
      "Iteration 4\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 34ms/step - accuracy: 0.6725 - loss: 0.8836 - val_accuracy: 0.6866 - val_loss: 0.8433\n",
      "Q 609+38  T 647  \u001b[91m☒ 651 \u001b[0m\n",
      "Q 434+341 T 775  \u001b[91m☒ 669 \u001b[0m\n",
      "Q 422+506 T 928  \u001b[91m☒ 944 \u001b[0m\n",
      "Q 148+467 T 615  \u001b[91m☒ 611 \u001b[0m\n",
      "Q 893+19  T 912  \u001b[91m☒ 919 \u001b[0m\n",
      "Q 741+9   T 750  \u001b[91m☒ 751 \u001b[0m\n",
      "Q 24+384  T 408  \u001b[91m☒ 411 \u001b[0m\n",
      "Q 684+796 T 1480 \u001b[91m☒ 1459\u001b[0m\n",
      "Q 186+688 T 874  \u001b[91m☒ 861 \u001b[0m\n",
      "Q 115+39  T 154  \u001b[92m☑ 154 \u001b[0m\n",
      "\n",
      "Iteration 5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 36ms/step - accuracy: 0.7109 - loss: 0.7835 - val_accuracy: 0.7174 - val_loss: 0.7623\n",
      "Q 15+360  T 375  \u001b[91m☒ 376 \u001b[0m\n",
      "Q 579+292 T 871  \u001b[91m☒ 876 \u001b[0m\n",
      "Q 8+902   T 910  \u001b[91m☒ 911 \u001b[0m\n",
      "Q 965+169 T 1134 \u001b[91m☒ 1133\u001b[0m\n",
      "Q 551+379 T 930  \u001b[91m☒ 923 \u001b[0m\n",
      "Q 61+296  T 357  \u001b[91m☒ 353 \u001b[0m\n",
      "Q 597+937 T 1534 \u001b[91m☒ 1634\u001b[0m\n",
      "Q 255+352 T 607  \u001b[91m☒ 606 \u001b[0m\n",
      "Q 352+493 T 845  \u001b[91m☒ 846 \u001b[0m\n",
      "Q 51+681  T 732  \u001b[91m☒ 737 \u001b[0m\n",
      "\n",
      "Iteration 6\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 34ms/step - accuracy: 0.7414 - loss: 0.7064 - val_accuracy: 0.7517 - val_loss: 0.6690\n",
      "Q 44+811  T 855  \u001b[91m☒ 858 \u001b[0m\n",
      "Q 271+146 T 417  \u001b[91m☒ 419 \u001b[0m\n",
      "Q 65+310  T 375  \u001b[91m☒ 379 \u001b[0m\n",
      "Q 352+895 T 1247 \u001b[91m☒ 1249\u001b[0m\n",
      "Q 394+0   T 394  \u001b[91m☒ 389 \u001b[0m\n",
      "Q 3+29    T 32   \u001b[91m☒ 33  \u001b[0m\n",
      "Q 695+52  T 747  \u001b[91m☒ 742 \u001b[0m\n",
      "Q 929+6   T 935  \u001b[91m☒ 934 \u001b[0m\n",
      "Q 934+10  T 944  \u001b[91m☒ 948 \u001b[0m\n",
      "Q 3+140   T 143  \u001b[91m☒ 144 \u001b[0m\n",
      "\n",
      "Iteration 7\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 37ms/step - accuracy: 0.7836 - loss: 0.5841 - val_accuracy: 0.8329 - val_loss: 0.4580\n",
      "Q 124+70  T 194  \u001b[91m☒ 193 \u001b[0m\n",
      "Q 607+19  T 626  \u001b[92m☑ 626 \u001b[0m\n",
      "Q 665+71  T 736  \u001b[92m☑ 736 \u001b[0m\n",
      "Q 838+579 T 1417 \u001b[91m☒ 1405\u001b[0m\n",
      "Q 44+64   T 108  \u001b[91m☒ 107 \u001b[0m\n",
      "Q 83+251  T 334  \u001b[91m☒ 335 \u001b[0m\n",
      "Q 82+92   T 174  \u001b[91m☒ 175 \u001b[0m\n",
      "Q 642+12  T 654  \u001b[92m☑ 654 \u001b[0m\n",
      "Q 687+401 T 1088 \u001b[91m☒ 1086\u001b[0m\n",
      "Q 110+78  T 188  \u001b[92m☑ 188 \u001b[0m\n",
      "\n",
      "Iteration 8\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 41ms/step - accuracy: 0.8875 - loss: 0.3328 - val_accuracy: 0.9104 - val_loss: 0.2839\n",
      "Q 11+347  T 358  \u001b[92m☑ 358 \u001b[0m\n",
      "Q 391+45  T 436  \u001b[92m☑ 436 \u001b[0m\n",
      "Q 784+477 T 1261 \u001b[91m☒ 1251\u001b[0m\n",
      "Q 47+234  T 281  \u001b[92m☑ 281 \u001b[0m\n",
      "Q 781+4   T 785  \u001b[92m☑ 785 \u001b[0m\n",
      "Q 1+638   T 639  \u001b[91m☒ 649 \u001b[0m\n",
      "Q 352+895 T 1247 \u001b[91m☒ 1248\u001b[0m\n",
      "Q 86+983  T 1069 \u001b[92m☑ 1069\u001b[0m\n",
      "Q 8+678   T 686  \u001b[92m☑ 686 \u001b[0m\n",
      "Q 857+380 T 1237 \u001b[91m☒ 1227\u001b[0m\n",
      "\n",
      "Iteration 9\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 37ms/step - accuracy: 0.9561 - loss: 0.1684 - val_accuracy: 0.9716 - val_loss: 0.1199\n",
      "Q 97+205  T 302  \u001b[92m☑ 302 \u001b[0m\n",
      "Q 280+959 T 1239 \u001b[91m☒ 1238\u001b[0m\n",
      "Q 420+868 T 1288 \u001b[92m☑ 1288\u001b[0m\n",
      "Q 15+826  T 841  \u001b[92m☑ 841 \u001b[0m\n",
      "Q 89+567  T 656  \u001b[92m☑ 656 \u001b[0m\n",
      "Q 16+10   T 26   \u001b[92m☑ 26  \u001b[0m\n",
      "Q 679+21  T 700  \u001b[91m☒ 600 \u001b[0m\n",
      "Q 560+113 T 673  \u001b[91m☒ 672 \u001b[0m\n",
      "Q 19+27   T 46   \u001b[92m☑ 46  \u001b[0m\n",
      "Q 125+953 T 1078 \u001b[92m☑ 1078\u001b[0m\n",
      "\n",
      "Iteration 10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 37ms/step - accuracy: 0.9711 - loss: 0.1111 - val_accuracy: 0.9856 - val_loss: 0.0689\n",
      "Q 53+474  T 527  \u001b[92m☑ 527 \u001b[0m\n",
      "Q 671+98  T 769  \u001b[92m☑ 769 \u001b[0m\n",
      "Q 43+465  T 508  \u001b[92m☑ 508 \u001b[0m\n",
      "Q 200+520 T 720  \u001b[91m☒ 700 \u001b[0m\n",
      "Q 528+7   T 535  \u001b[92m☑ 535 \u001b[0m\n",
      "Q 722+34  T 756  \u001b[92m☑ 756 \u001b[0m\n",
      "Q 41+23   T 64   \u001b[92m☑ 64  \u001b[0m\n",
      "Q 48+95   T 143  \u001b[92m☑ 143 \u001b[0m\n",
      "Q 718+34  T 752  \u001b[92m☑ 752 \u001b[0m\n",
      "Q 992+4   T 996  \u001b[92m☑ 996 \u001b[0m\n",
      "\n",
      "Iteration 11\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 35ms/step - accuracy: 0.9825 - loss: 0.0719 - val_accuracy: 0.9818 - val_loss: 0.0697\n",
      "Q 21+956  T 977  \u001b[92m☑ 977 \u001b[0m\n",
      "Q 65+601  T 666  \u001b[92m☑ 666 \u001b[0m\n",
      "Q 848+64  T 912  \u001b[92m☑ 912 \u001b[0m\n",
      "Q 285+67  T 352  \u001b[92m☑ 352 \u001b[0m\n",
      "Q 33+166  T 199  \u001b[91m☒ 299 \u001b[0m\n",
      "Q 664+793 T 1457 \u001b[92m☑ 1457\u001b[0m\n",
      "Q 551+81  T 632  \u001b[92m☑ 632 \u001b[0m\n",
      "Q 899+26  T 925  \u001b[92m☑ 925 \u001b[0m\n",
      "Q 48+733  T 781  \u001b[92m☑ 781 \u001b[0m\n",
      "Q 718+281 T 999  \u001b[92m☑ 999 \u001b[0m\n",
      "\n",
      "Iteration 12\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 42ms/step - accuracy: 0.9849 - loss: 0.0601 - val_accuracy: 0.9775 - val_loss: 0.0713\n",
      "Q 973+49  T 1022 \u001b[92m☑ 1022\u001b[0m\n",
      "Q 509+777 T 1286 \u001b[92m☑ 1286\u001b[0m\n",
      "Q 840+586 T 1426 \u001b[92m☑ 1426\u001b[0m\n",
      "Q 532+745 T 1277 \u001b[92m☑ 1277\u001b[0m\n",
      "Q 28+98   T 126  \u001b[92m☑ 126 \u001b[0m\n",
      "Q 31+967  T 998  \u001b[92m☑ 998 \u001b[0m\n",
      "Q 67+430  T 497  \u001b[92m☑ 497 \u001b[0m\n",
      "Q 252+80  T 332  \u001b[92m☑ 332 \u001b[0m\n",
      "Q 126+273 T 399  \u001b[92m☑ 399 \u001b[0m\n",
      "Q 579+309 T 888  \u001b[92m☑ 888 \u001b[0m\n",
      "\n",
      "Iteration 13\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 34ms/step - accuracy: 0.9854 - loss: 0.0537 - val_accuracy: 0.9908 - val_loss: 0.0379\n",
      "Q 1+290   T 291  \u001b[91m☒ 292 \u001b[0m\n",
      "Q 4+878   T 882  \u001b[92m☑ 882 \u001b[0m\n",
      "Q 77+871  T 948  \u001b[92m☑ 948 \u001b[0m\n",
      "Q 114+58  T 172  \u001b[92m☑ 172 \u001b[0m\n",
      "Q 180+37  T 217  \u001b[92m☑ 217 \u001b[0m\n",
      "Q 689+0   T 689  \u001b[92m☑ 689 \u001b[0m\n",
      "Q 527+7   T 534  \u001b[92m☑ 534 \u001b[0m\n",
      "Q 724+910 T 1634 \u001b[92m☑ 1634\u001b[0m\n",
      "Q 207+653 T 860  \u001b[92m☑ 860 \u001b[0m\n",
      "Q 26+489  T 515  \u001b[92m☑ 515 \u001b[0m\n",
      "\n",
      "Iteration 14\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 34ms/step - accuracy: 0.9924 - loss: 0.0325 - val_accuracy: 0.9952 - val_loss: 0.0215\n",
      "Q 97+69   T 166  \u001b[92m☑ 166 \u001b[0m\n",
      "Q 603+50  T 653  \u001b[92m☑ 653 \u001b[0m\n",
      "Q 91+639  T 730  \u001b[92m☑ 730 \u001b[0m\n",
      "Q 30+143  T 173  \u001b[92m☑ 173 \u001b[0m\n",
      "Q 185+7   T 192  \u001b[92m☑ 192 \u001b[0m\n",
      "Q 471+99  T 570  \u001b[92m☑ 570 \u001b[0m\n",
      "Q 22+633  T 655  \u001b[92m☑ 655 \u001b[0m\n",
      "Q 584+877 T 1461 \u001b[92m☑ 1461\u001b[0m\n",
      "Q 32+939  T 971  \u001b[92m☑ 971 \u001b[0m\n",
      "Q 95+427  T 522  \u001b[92m☑ 522 \u001b[0m\n",
      "\n",
      "Iteration 15\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 36ms/step - accuracy: 0.9884 - loss: 0.0414 - val_accuracy: 0.9970 - val_loss: 0.0165\n",
      "Q 535+11  T 546  \u001b[92m☑ 546 \u001b[0m\n",
      "Q 720+133 T 853  \u001b[92m☑ 853 \u001b[0m\n",
      "Q 6+953   T 959  \u001b[91m☒ 969 \u001b[0m\n",
      "Q 560+94  T 654  \u001b[92m☑ 654 \u001b[0m\n",
      "Q 71+356  T 427  \u001b[92m☑ 427 \u001b[0m\n",
      "Q 638+88  T 726  \u001b[92m☑ 726 \u001b[0m\n",
      "Q 358+86  T 444  \u001b[92m☑ 444 \u001b[0m\n",
      "Q 99+788  T 887  \u001b[92m☑ 887 \u001b[0m\n",
      "Q 113+368 T 481  \u001b[92m☑ 481 \u001b[0m\n",
      "Q 438+710 T 1148 \u001b[92m☑ 1148\u001b[0m\n",
      "\n",
      "Iteration 16\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 40ms/step - accuracy: 0.9897 - loss: 0.0356 - val_accuracy: 0.9973 - val_loss: 0.0138\n",
      "Q 706+56  T 762  \u001b[92m☑ 762 \u001b[0m\n",
      "Q 848+27  T 875  \u001b[92m☑ 875 \u001b[0m\n",
      "Q 33+857  T 890  \u001b[92m☑ 890 \u001b[0m\n",
      "Q 6+128   T 134  \u001b[92m☑ 134 \u001b[0m\n",
      "Q 149+72  T 221  \u001b[92m☑ 221 \u001b[0m\n",
      "Q 326+982 T 1308 \u001b[92m☑ 1308\u001b[0m\n",
      "Q 93+60   T 153  \u001b[92m☑ 153 \u001b[0m\n",
      "Q 677+40  T 717  \u001b[92m☑ 717 \u001b[0m\n",
      "Q 630+753 T 1383 \u001b[92m☑ 1383\u001b[0m\n",
      "Q 776+77  T 853  \u001b[92m☑ 853 \u001b[0m\n",
      "\n",
      "Iteration 17\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 36ms/step - accuracy: 0.9911 - loss: 0.0315 - val_accuracy: 0.9970 - val_loss: 0.0151\n",
      "Q 623+36  T 659  \u001b[92m☑ 659 \u001b[0m\n",
      "Q 714+674 T 1388 \u001b[92m☑ 1388\u001b[0m\n",
      "Q 22+897  T 919  \u001b[92m☑ 919 \u001b[0m\n",
      "Q 387+651 T 1038 \u001b[92m☑ 1038\u001b[0m\n",
      "Q 367+60  T 427  \u001b[92m☑ 427 \u001b[0m\n",
      "Q 94+968  T 1062 \u001b[92m☑ 1062\u001b[0m\n",
      "Q 63+938  T 1001 \u001b[92m☑ 1001\u001b[0m\n",
      "Q 519+17  T 536  \u001b[92m☑ 536 \u001b[0m\n",
      "Q 793+468 T 1261 \u001b[92m☑ 1261\u001b[0m\n",
      "Q 481+32  T 513  \u001b[92m☑ 513 \u001b[0m\n",
      "\n",
      "Iteration 18\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 34ms/step - accuracy: 0.9928 - loss: 0.0271 - val_accuracy: 0.9973 - val_loss: 0.0131\n",
      "Q 574+893 T 1467 \u001b[92m☑ 1467\u001b[0m\n",
      "Q 867+99  T 966  \u001b[92m☑ 966 \u001b[0m\n",
      "Q 11+229  T 240  \u001b[92m☑ 240 \u001b[0m\n",
      "Q 99+912  T 1011 \u001b[92m☑ 1011\u001b[0m\n",
      "Q 68+108  T 176  \u001b[92m☑ 176 \u001b[0m\n",
      "Q 28+305  T 333  \u001b[92m☑ 333 \u001b[0m\n",
      "Q 323+9   T 332  \u001b[92m☑ 332 \u001b[0m\n",
      "Q 15+407  T 422  \u001b[92m☑ 422 \u001b[0m\n",
      "Q 0+975   T 975  \u001b[92m☑ 975 \u001b[0m\n",
      "Q 694+4   T 698  \u001b[92m☑ 698 \u001b[0m\n",
      "\n",
      "Iteration 19\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 42ms/step - accuracy: 0.9928 - loss: 0.0265 - val_accuracy: 0.9973 - val_loss: 0.0139\n",
      "Q 422+67  T 489  \u001b[92m☑ 489 \u001b[0m\n",
      "Q 412+232 T 644  \u001b[92m☑ 644 \u001b[0m\n",
      "Q 152+845 T 997  \u001b[92m☑ 997 \u001b[0m\n",
      "Q 547+749 T 1296 \u001b[92m☑ 1296\u001b[0m\n",
      "Q 852+64  T 916  \u001b[92m☑ 916 \u001b[0m\n",
      "Q 943+35  T 978  \u001b[92m☑ 978 \u001b[0m\n",
      "Q 815+547 T 1362 \u001b[92m☑ 1362\u001b[0m\n",
      "Q 5+499   T 504  \u001b[92m☑ 504 \u001b[0m\n",
      "Q 32+447  T 479  \u001b[92m☑ 479 \u001b[0m\n",
      "Q 547+0   T 547  \u001b[92m☑ 547 \u001b[0m\n",
      "\n",
      "Iteration 20\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 34ms/step - accuracy: 0.9916 - loss: 0.0295 - val_accuracy: 0.9922 - val_loss: 0.0279\n",
      "Q 291+92  T 383  \u001b[92m☑ 383 \u001b[0m\n",
      "Q 37+917  T 954  \u001b[92m☑ 954 \u001b[0m\n",
      "Q 46+924  T 970  \u001b[92m☑ 970 \u001b[0m\n",
      "Q 467+76  T 543  \u001b[92m☑ 543 \u001b[0m\n",
      "Q 337+5   T 342  \u001b[92m☑ 342 \u001b[0m\n",
      "Q 28+98   T 126  \u001b[92m☑ 126 \u001b[0m\n",
      "Q 836+19  T 855  \u001b[92m☑ 855 \u001b[0m\n",
      "Q 28+16   T 44   \u001b[92m☑ 44  \u001b[0m\n",
      "Q 91+531  T 622  \u001b[92m☑ 622 \u001b[0m\n",
      "Q 13+847  T 860  \u001b[92m☑ 860 \u001b[0m\n",
      "\n",
      "Iteration 21\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 36ms/step - accuracy: 0.9956 - loss: 0.0171 - val_accuracy: 0.9822 - val_loss: 0.0507\n",
      "Q 69+91   T 160  \u001b[92m☑ 160 \u001b[0m\n",
      "Q 93+620  T 713  \u001b[92m☑ 713 \u001b[0m\n",
      "Q 718+34  T 752  \u001b[92m☑ 752 \u001b[0m\n",
      "Q 63+296  T 359  \u001b[92m☑ 359 \u001b[0m\n",
      "Q 608+666 T 1274 \u001b[92m☑ 1274\u001b[0m\n",
      "Q 940+838 T 1778 \u001b[92m☑ 1778\u001b[0m\n",
      "Q 0+15    T 15   \u001b[92m☑ 15  \u001b[0m\n",
      "Q 73+101  T 174  \u001b[92m☑ 174 \u001b[0m\n",
      "Q 72+318  T 390  \u001b[92m☑ 390 \u001b[0m\n",
      "Q 61+811  T 872  \u001b[92m☑ 872 \u001b[0m\n",
      "\n",
      "Iteration 22\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 40ms/step - accuracy: 0.9965 - loss: 0.0153 - val_accuracy: 0.9991 - val_loss: 0.0061\n",
      "Q 41+837  T 878  \u001b[92m☑ 878 \u001b[0m\n",
      "Q 625+479 T 1104 \u001b[92m☑ 1104\u001b[0m\n",
      "Q 69+556  T 625  \u001b[92m☑ 625 \u001b[0m\n",
      "Q 81+34   T 115  \u001b[92m☑ 115 \u001b[0m\n",
      "Q 492+27  T 519  \u001b[92m☑ 519 \u001b[0m\n",
      "Q 0+425   T 425  \u001b[92m☑ 425 \u001b[0m\n",
      "Q 84+563  T 647  \u001b[92m☑ 647 \u001b[0m\n",
      "Q 185+55  T 240  \u001b[92m☑ 240 \u001b[0m\n",
      "Q 55+81   T 136  \u001b[92m☑ 136 \u001b[0m\n",
      "Q 46+928  T 974  \u001b[92m☑ 974 \u001b[0m\n",
      "\n",
      "Iteration 23\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 36ms/step - accuracy: 0.9919 - loss: 0.0287 - val_accuracy: 0.9903 - val_loss: 0.0285\n",
      "Q 759+339 T 1098 \u001b[92m☑ 1098\u001b[0m\n",
      "Q 135+51  T 186  \u001b[92m☑ 186 \u001b[0m\n",
      "Q 664+418 T 1082 \u001b[92m☑ 1082\u001b[0m\n",
      "Q 1+511   T 512  \u001b[92m☑ 512 \u001b[0m\n",
      "Q 34+68   T 102  \u001b[92m☑ 102 \u001b[0m\n",
      "Q 583+7   T 590  \u001b[92m☑ 590 \u001b[0m\n",
      "Q 815+421 T 1236 \u001b[92m☑ 1236\u001b[0m\n",
      "Q 186+9   T 195  \u001b[92m☑ 195 \u001b[0m\n",
      "Q 15+826  T 841  \u001b[92m☑ 841 \u001b[0m\n",
      "Q 754+19  T 773  \u001b[92m☑ 773 \u001b[0m\n",
      "\n",
      "Iteration 24\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 40ms/step - accuracy: 0.9966 - loss: 0.0132 - val_accuracy: 0.9909 - val_loss: 0.0290\n",
      "Q 138+2   T 140  \u001b[92m☑ 140 \u001b[0m\n",
      "Q 83+369  T 452  \u001b[92m☑ 452 \u001b[0m\n",
      "Q 134+314 T 448  \u001b[92m☑ 448 \u001b[0m\n",
      "Q 25+688  T 713  \u001b[92m☑ 713 \u001b[0m\n",
      "Q 622+28  T 650  \u001b[92m☑ 650 \u001b[0m\n",
      "Q 401+12  T 413  \u001b[92m☑ 413 \u001b[0m\n",
      "Q 65+11   T 76   \u001b[92m☑ 76  \u001b[0m\n",
      "Q 38+649  T 687  \u001b[92m☑ 687 \u001b[0m\n",
      "Q 145+19  T 164  \u001b[92m☑ 164 \u001b[0m\n",
      "Q 21+693  T 714  \u001b[92m☑ 714 \u001b[0m\n",
      "\n",
      "Iteration 25\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 34ms/step - accuracy: 0.9935 - loss: 0.0231 - val_accuracy: 0.9987 - val_loss: 0.0073\n",
      "Q 760+886 T 1646 \u001b[92m☑ 1646\u001b[0m\n",
      "Q 45+7    T 52   \u001b[92m☑ 52  \u001b[0m\n",
      "Q 848+1   T 849  \u001b[92m☑ 849 \u001b[0m\n",
      "Q 6+391   T 397  \u001b[92m☑ 397 \u001b[0m\n",
      "Q 203+65  T 268  \u001b[92m☑ 268 \u001b[0m\n",
      "Q 81+87   T 168  \u001b[92m☑ 168 \u001b[0m\n",
      "Q 815+384 T 1199 \u001b[92m☑ 1199\u001b[0m\n",
      "Q 212+7   T 219  \u001b[92m☑ 219 \u001b[0m\n",
      "Q 210+688 T 898  \u001b[92m☑ 898 \u001b[0m\n",
      "Q 705+862 T 1567 \u001b[92m☑ 1567\u001b[0m\n",
      "\n",
      "Iteration 26\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 42ms/step - accuracy: 0.9943 - loss: 0.0203 - val_accuracy: 0.9990 - val_loss: 0.0062\n",
      "Q 718+281 T 999  \u001b[92m☑ 999 \u001b[0m\n",
      "Q 2+760   T 762  \u001b[92m☑ 762 \u001b[0m\n",
      "Q 946+225 T 1171 \u001b[92m☑ 1171\u001b[0m\n",
      "Q 48+786  T 834  \u001b[92m☑ 834 \u001b[0m\n",
      "Q 348+2   T 350  \u001b[92m☑ 350 \u001b[0m\n",
      "Q 5+297   T 302  \u001b[92m☑ 302 \u001b[0m\n",
      "Q 43+196  T 239  \u001b[92m☑ 239 \u001b[0m\n",
      "Q 321+301 T 622  \u001b[92m☑ 622 \u001b[0m\n",
      "Q 56+750  T 806  \u001b[92m☑ 806 \u001b[0m\n",
      "Q 946+733 T 1679 \u001b[92m☑ 1679\u001b[0m\n",
      "\n",
      "Iteration 27\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 35ms/step - accuracy: 0.9955 - loss: 0.0159 - val_accuracy: 0.9938 - val_loss: 0.0246\n",
      "Q 85+139  T 224  \u001b[92m☑ 224 \u001b[0m\n",
      "Q 49+857  T 906  \u001b[92m☑ 906 \u001b[0m\n",
      "Q 334+68  T 402  \u001b[92m☑ 402 \u001b[0m\n",
      "Q 28+165  T 193  \u001b[92m☑ 193 \u001b[0m\n",
      "Q 128+500 T 628  \u001b[92m☑ 628 \u001b[0m\n",
      "Q 52+705  T 757  \u001b[92m☑ 757 \u001b[0m\n",
      "Q 955+442 T 1397 \u001b[92m☑ 1397\u001b[0m\n",
      "Q 17+508  T 525  \u001b[92m☑ 525 \u001b[0m\n",
      "Q 141+27  T 168  \u001b[92m☑ 168 \u001b[0m\n",
      "Q 412+243 T 655  \u001b[92m☑ 655 \u001b[0m\n",
      "\n",
      "Iteration 28\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 42ms/step - accuracy: 0.9972 - loss: 0.0109 - val_accuracy: 0.9952 - val_loss: 0.0183\n",
      "Q 968+703 T 1671 \u001b[92m☑ 1671\u001b[0m\n",
      "Q 46+638  T 684  \u001b[92m☑ 684 \u001b[0m\n",
      "Q 265+641 T 906  \u001b[91m☒ 806 \u001b[0m\n",
      "Q 919+79  T 998  \u001b[92m☑ 998 \u001b[0m\n",
      "Q 87+992  T 1079 \u001b[92m☑ 1079\u001b[0m\n",
      "Q 62+825  T 887  \u001b[92m☑ 887 \u001b[0m\n",
      "Q 150+12  T 162  \u001b[92m☑ 162 \u001b[0m\n",
      "Q 68+433  T 501  \u001b[92m☑ 501 \u001b[0m\n",
      "Q 566+659 T 1225 \u001b[92m☑ 1225\u001b[0m\n",
      "Q 54+30   T 84   \u001b[92m☑ 84  \u001b[0m\n",
      "\n",
      "Iteration 29\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 35ms/step - accuracy: 0.9962 - loss: 0.0148 - val_accuracy: 0.9991 - val_loss: 0.0047\n",
      "Q 981+14  T 995  \u001b[92m☑ 995 \u001b[0m\n",
      "Q 187+946 T 1133 \u001b[92m☑ 1133\u001b[0m\n",
      "Q 20+120  T 140  \u001b[92m☑ 140 \u001b[0m\n",
      "Q 442+36  T 478  \u001b[92m☑ 478 \u001b[0m\n",
      "Q 32+394  T 426  \u001b[92m☑ 426 \u001b[0m\n",
      "Q 46+59   T 105  \u001b[92m☑ 105 \u001b[0m\n",
      "Q 638+22  T 660  \u001b[92m☑ 660 \u001b[0m\n",
      "Q 528+6   T 534  \u001b[92m☑ 534 \u001b[0m\n",
      "Q 401+12  T 413  \u001b[92m☑ 413 \u001b[0m\n",
      "Q 873+97  T 970  \u001b[92m☑ 970 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Training parameters.\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "# Formatting characters for results display.\n",
    "green_color = \"\\033[92m\"\n",
    "red_color = \"\\033[91m\"\n",
    "end_char = \"\\033[0m\"\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for epoch in range(1, epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = np.argmax(model.predict(rowx, verbose=0), axis=-1)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
    "        print(\"T\", correct, end=\" \")\n",
    "        if correct == guess:\n",
    "            print(f\"{green_color}☑ {guess}{end_char}\")\n",
    "        else:\n",
    "            print(f\"{red_color}☒ {guess}{end_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ea433-1ca5-4b89-aa6f-6059966379f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
