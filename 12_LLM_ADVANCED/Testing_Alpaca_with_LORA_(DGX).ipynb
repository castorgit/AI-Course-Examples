{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ada78a-ad89-4c63-ab28-fb4bc8b8e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import sentencepiece\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2546dc49-aec4-4de9-9c48-79625077fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secrets.env found and loaded\n"
     ]
    }
   ],
   "source": [
    "# load secret tokens\n",
    "\n",
    "env_path = Path(\"../secrets.env\").resolve()\n",
    "\n",
    "if not env_path.exists():\n",
    "    raise FileNotFoundError(f\"secrets.env not found at: {env_path}\")\n",
    "\n",
    "load_dotenv(env_path)  # This reads the .env file\n",
    "HF_token = os.getenv(\"HUGGING_FACE_token\")\n",
    "print(\"secrets.env found and loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8393eef1-2300-4723-9d2c-fb12ad009c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440080cf1e024107bcd078ceaa9cedaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaume/DL/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "lora_path = \"./models/lora_apaca_llama2\"\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id, \n",
    "        token=HF_token,\n",
    "        dtype=torch.bfloat16,                       # trying to reduce space as it reaches GPU mem limit easily\n",
    "        cache_dir=\"./.cache/huggingface\").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=HF_token,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30871188-ce90-4bd9-b182-9e9e23a2b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpaca_prompt(instruction, input_text=\"\"):\n",
    "    if input_text:\n",
    "        return f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8920b6ea-260a-4caa-aa26-35b748f543cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Explain what reinforcement learning is\n",
      "\n",
      "### Response:\n",
      "t Reinforcement Learning (RL) is a type of machine learning algorithm that uses trial and error to improve performance on a task. The RL algorithm learns through trial and error, using rewards and punishments to guide its decisions. It takes into account the current state of the environment, the possible actions it can take, and the resulting consequences. As it explores different actions in the environment, the RL algorithm updates its policy over time until it finds an optimal solution.\n"
     ]
    }
   ],
   "source": [
    "prompt = alpaca_prompt(\n",
    "    instruction=\"Explain what reinforcement learning is\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5bc5fd-7cf9-40ea-a0d9-ed5832d163da",
   "metadata": {},
   "source": [
    "Response must follow instructions, self-eplanatory. Tone Helpful and explanatory and Mentions steps, examples, structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afcb54d-6f9c-4f1b-be72-444a36c2360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write a Python function that checks if a number is prime\n",
      "\n",
      "### Response:\n",
      " - def check_prime(num): \n",
      "     for i in range(2, num): \n",
      "         if (num % i) == 0: \n",
      "             return False\n",
      "     else: \n",
      "         return True\n"
     ]
    }
   ],
   "source": [
    "prompt = alpaca_prompt(\n",
    "    \"Write a Python function that checks if a number is prime\"\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97227b1f-8658-4014-9428-4ea526738994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5f45a46a8f46f3a5d8358cda2a54c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE:\n",
      " Explain gradient descent in exactly 3 bullet points. Do not use equations. Do not write more than one sentence per bullet.\n",
      "Gradient descent is a method to minimize a function of many variables by iteratively changing the variables until the function value is minimized.\n",
      "Gradient descent is an optimization technique that involves iteratively changing the values of variables in a function until the function value is minimized.\n",
      "Gradient descent is a mathematical optimization technique that involves iteratively changing the values of variables in a function until the function value is minimized.\n",
      "Gradient descent is an optimization technique that involves iteratively changing the values of variables in a function until the function value is minimized.\n",
      "Gradient descent is a mathematical optimization technique that involves iteratively changing the values of variables in a function until the function value is minimized. It is a type of gradient-\n",
      "\n",
      "LORA:\n",
      " Explain gradient descent in exactly 3 bullet points. Do not use equations. Do not write more than one sentence per bullet.\n",
      "\n",
      "3. Gradient descent is a technique used to minimize a function by iteratively improving the approximation of the function's minimum point.\n",
      "\n",
      "3. Gradient descent is a technique used to find the minimum of a function by finding the direction of steepest ascent and moving in that direction.\n",
      "\n",
      "3. Gradient descent is a technique used to find the minimum of a function by finding the direction of steepest descent and moving in that direction.\n"
     ]
    }
   ],
   "source": [
    "# Base model vs Alpaca trained\n",
    "prompt = \"Explain gradient descent in exactly 3 bullet points. Do not use equations. Do not write more than one sentence per bullet.\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id, \n",
    "        token=HF_token,\n",
    "        dtype=torch.bfloat16,                       # trying to reduce space as it reaches GPU mem limit easily\n",
    "        cache_dir=\"./.cache/huggingface\").to(\"cuda\")\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out_base = base_model.generate(**inputs, max_new_tokens=150)\n",
    "out_lora = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "print(\"BASE:\\n\", tokenizer.decode(out_base[0], skip_special_tokens=True))\n",
    "print(\"\\nLORA:\\n\", tokenizer.decode(out_lora[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c24bda-34f8-490e-aa24-0a7589d0491b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
