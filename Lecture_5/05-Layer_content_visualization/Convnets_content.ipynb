{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing internal layers in a simple ConvNet\n",
    "\n",
    "**Description:** Visualizes the content of the intermediate layers of a convnet <br>\n",
    "**Dataset:** We use simple circles and triangles from Kaggle <br>\n",
    "\n",
    "\n",
    "https://github.com/gabrielpierobon/cnnshapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # to avoid warning messages\n",
    "\n",
    "import glob\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import imageio as im\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images and split in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# folder structure \n",
    "#Notebook\n",
    "#    !------ data\n",
    "#                  !-------------squares\n",
    "#                  !-------------circles\n",
    "#                  !-------------triangles\n",
    "#\n",
    "data_datagen = ImageDataGenerator(rescale = 1./255, validation_split=0.2)\n",
    "\n",
    "training_set= data_datagen.flow_from_directory(  directory = './data/',\n",
    "                                                 target_size = (28, 28),\n",
    "                                                 batch_size = 16,\n",
    "                                                 subset='training',\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "validation_set=data_datagen.flow_from_directory( directory = './data/',\n",
    "                                                 target_size = (28, 28),\n",
    "                                                 batch_size = 16,\n",
    "                                                 subset='validation',\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = training_set.next()  \n",
    "plt.figure(figsize=(20,10))\n",
    "columns = 4\n",
    "batch_size = 16\n",
    "for i in range(0,batch_size):\n",
    "    image = x[i]\n",
    "    fil = int((batch_size / columns )+1)\n",
    "    col = int(i +1)\n",
    "    plt.subplot(fil, columns, col)\n",
    "#    plt.subplot(16/ columns + 1, columns, i + 1)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition using sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 13, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               819712    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 886,819\n",
      "Trainable params: 886,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape = (28, 28, 3), activation = 'relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5)) # antes era 0.25\n",
    "\n",
    "# layer 2\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5)) # antes era 0.25\n",
    "\n",
    "# layer 3\n",
    "#model.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "#model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5)) # antes era 0.25\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# MLP at the end - classifier\n",
    "model.add(Dense(units = 512, activation = 'relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(units = 3, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop',\n",
    "                   loss = 'categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=\"best_weights.hdf5\", \n",
    "                               monitor = 'val_accuracy',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2626 - accuracy: 0.3688\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.33333, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 1.2626 - accuracy: 0.3688 - val_loss: 1.0996 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1164 - accuracy: 0.3187\n",
      "Epoch 00002: val_accuracy did not improve from 0.33333\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 1.1164 - accuracy: 0.3187 - val_loss: 1.1005 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1121 - accuracy: 0.3250\n",
      "Epoch 00003: val_accuracy did not improve from 0.33333\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 1.1121 - accuracy: 0.3250 - val_loss: 1.0989 - val_accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1056 - accuracy: 0.3438\n",
      "Epoch 00004: val_accuracy improved from 0.33333 to 0.53333, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 1.1056 - accuracy: 0.3438 - val_loss: 1.0960 - val_accuracy: 0.5333\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1013 - accuracy: 0.3812\n",
      "Epoch 00005: val_accuracy did not improve from 0.53333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.1013 - accuracy: 0.3812 - val_loss: 1.0956 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0925 - accuracy: 0.3500\n",
      "Epoch 00006: val_accuracy did not improve from 0.53333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.0925 - accuracy: 0.3500 - val_loss: 1.0963 - val_accuracy: 0.3333\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1124 - accuracy: 0.3250\n",
      "Epoch 00007: val_accuracy did not improve from 0.53333\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 1.1124 - accuracy: 0.3250 - val_loss: 1.0854 - val_accuracy: 0.4167\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0786 - accuracy: 0.3812\n",
      "Epoch 00008: val_accuracy did not improve from 0.53333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.0786 - accuracy: 0.3812 - val_loss: 1.0792 - val_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0942 - accuracy: 0.4062\n",
      "Epoch 00009: val_accuracy improved from 0.53333 to 0.56667, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 1.0942 - accuracy: 0.4062 - val_loss: 1.0067 - val_accuracy: 0.5667\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0666 - accuracy: 0.5562\n",
      "Epoch 00010: val_accuracy improved from 0.56667 to 0.63333, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 1.0666 - accuracy: 0.5562 - val_loss: 0.9198 - val_accuracy: 0.6333\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9773 - accuracy: 0.5188\n",
      "Epoch 00011: val_accuracy improved from 0.63333 to 0.80000, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.9773 - accuracy: 0.5188 - val_loss: 0.8477 - val_accuracy: 0.8000\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8632 - accuracy: 0.5813\n",
      "Epoch 00012: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8632 - accuracy: 0.5813 - val_loss: 0.7600 - val_accuracy: 0.5667\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8594 - accuracy: 0.6250\n",
      "Epoch 00013: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8594 - accuracy: 0.6250 - val_loss: 0.7275 - val_accuracy: 0.7833\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8163 - accuracy: 0.6500\n",
      "Epoch 00014: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8163 - accuracy: 0.6500 - val_loss: 0.7005 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7233 - accuracy: 0.7312\n",
      "Epoch 00015: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.7233 - accuracy: 0.7312 - val_loss: 0.6101 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.7750\n",
      "Epoch 00016: val_accuracy improved from 0.80000 to 0.81667, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.6553 - accuracy: 0.7750 - val_loss: 0.5810 - val_accuracy: 0.8167\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7551 - accuracy: 0.7250\n",
      "Epoch 00017: val_accuracy improved from 0.81667 to 0.83333, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.7551 - accuracy: 0.7250 - val_loss: 0.5968 - val_accuracy: 0.8333\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5998 - accuracy: 0.7688\n",
      "Epoch 00018: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.5998 - accuracy: 0.7688 - val_loss: 1.1555 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7493 - accuracy: 0.7312\n",
      "Epoch 00019: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.7493 - accuracy: 0.7312 - val_loss: 0.4833 - val_accuracy: 0.8167\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5484 - accuracy: 0.7688\n",
      "Epoch 00020: val_accuracy improved from 0.83333 to 0.85000, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.5484 - accuracy: 0.7688 - val_loss: 0.4466 - val_accuracy: 0.8500\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.7750\n",
      "Epoch 00021: val_accuracy improved from 0.85000 to 0.86667, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.5256 - accuracy: 0.7750 - val_loss: 0.4430 - val_accuracy: 0.8667\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6004 - accuracy: 0.7937\n",
      "Epoch 00022: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.6004 - accuracy: 0.7937 - val_loss: 0.4161 - val_accuracy: 0.8667\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.7875\n",
      "Epoch 00023: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.5784 - accuracy: 0.7875 - val_loss: 0.8430 - val_accuracy: 0.7333\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4699 - accuracy: 0.8438\n",
      "Epoch 00024: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.4699 - accuracy: 0.8438 - val_loss: 0.4055 - val_accuracy: 0.8667\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3863 - accuracy: 0.8750\n",
      "Epoch 00025: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.3863 - accuracy: 0.8750 - val_loss: 0.4263 - val_accuracy: 0.8667\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 0.8813\n",
      "Epoch 00026: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.3778 - accuracy: 0.8813 - val_loss: 0.3964 - val_accuracy: 0.8667\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3672 - accuracy: 0.8438\n",
      "Epoch 00027: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.3672 - accuracy: 0.8438 - val_loss: 0.3960 - val_accuracy: 0.8667\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.8500\n",
      "Epoch 00028: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.4617 - accuracy: 0.8500 - val_loss: 0.4167 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3900 - accuracy: 0.8625\n",
      "Epoch 00029: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.3900 - accuracy: 0.8625 - val_loss: 0.4806 - val_accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9000\n",
      "Epoch 00030: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2819 - accuracy: 0.9000 - val_loss: 0.4648 - val_accuracy: 0.8500\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.8687\n",
      "Epoch 00031: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.3588 - accuracy: 0.8687 - val_loss: 0.3647 - val_accuracy: 0.8667\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4312 - accuracy: 0.8562\n",
      "Epoch 00032: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.4312 - accuracy: 0.8562 - val_loss: 0.4266 - val_accuracy: 0.8333\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9125\n",
      "Epoch 00033: val_accuracy improved from 0.86667 to 0.90000, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.2014 - accuracy: 0.9125 - val_loss: 0.3870 - val_accuracy: 0.9000\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.9500\n",
      "Epoch 00034: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1824 - accuracy: 0.9500 - val_loss: 0.5439 - val_accuracy: 0.8833\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9250\n",
      "Epoch 00035: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2425 - accuracy: 0.9250 - val_loss: 0.4076 - val_accuracy: 0.8833\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9187\n",
      "Epoch 00036: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2542 - accuracy: 0.9187 - val_loss: 0.5034 - val_accuracy: 0.8833\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9062\n",
      "Epoch 00037: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2184 - accuracy: 0.9062 - val_loss: 0.3739 - val_accuracy: 0.8833\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.9000\n",
      "Epoch 00038: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2617 - accuracy: 0.9000 - val_loss: 0.3952 - val_accuracy: 0.9000\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9500\n",
      "Epoch 00039: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1473 - accuracy: 0.9500 - val_loss: 0.3741 - val_accuracy: 0.8667\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9187\n",
      "Epoch 00040: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1980 - accuracy: 0.9187 - val_loss: 0.6145 - val_accuracy: 0.8000\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.8875\n",
      "Epoch 00041: val_accuracy improved from 0.90000 to 0.91667, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.3455 - accuracy: 0.8875 - val_loss: 0.3515 - val_accuracy: 0.9167\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9563\n",
      "Epoch 00042: val_accuracy improved from 0.91667 to 0.93333, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.1436 - accuracy: 0.9563 - val_loss: 0.3020 - val_accuracy: 0.9333\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9563\n",
      "Epoch 00043: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1565 - accuracy: 0.9563 - val_loss: 0.2897 - val_accuracy: 0.9000\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2353 - accuracy: 0.9000\n",
      "Epoch 00044: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2353 - accuracy: 0.9000 - val_loss: 0.2679 - val_accuracy: 0.9333\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9187\n",
      "Epoch 00045: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1586 - accuracy: 0.9187 - val_loss: 0.3443 - val_accuracy: 0.8667\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9625\n",
      "Epoch 00046: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1073 - accuracy: 0.9625 - val_loss: 0.3941 - val_accuracy: 0.8833\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9563\n",
      "Epoch 00047: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.1054 - accuracy: 0.9563 - val_loss: 0.2603 - val_accuracy: 0.9167\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8813\n",
      "Epoch 00048: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.4268 - accuracy: 0.8813 - val_loss: 0.3603 - val_accuracy: 0.9167\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9812\n",
      "Epoch 00049: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0723 - accuracy: 0.9812 - val_loss: 0.2659 - val_accuracy: 0.9333\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9750\n",
      "Epoch 00050: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1037 - accuracy: 0.9750 - val_loss: 0.2753 - val_accuracy: 0.9167\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9875\n",
      "Epoch 00051: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0493 - accuracy: 0.9875 - val_loss: 0.4169 - val_accuracy: 0.8667\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9750\n",
      "Epoch 00052: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0972 - accuracy: 0.9750 - val_loss: 0.4727 - val_accuracy: 0.8667\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9563\n",
      "Epoch 00053: val_accuracy improved from 0.93333 to 0.95000, saving model to best_weights.hdf5\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.1284 - accuracy: 0.9563 - val_loss: 0.2935 - val_accuracy: 0.9500\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9875\n",
      "Epoch 00054: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0682 - accuracy: 0.9875 - val_loss: 0.2916 - val_accuracy: 0.9333\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9625\n",
      "Epoch 00055: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1486 - accuracy: 0.9625 - val_loss: 0.3373 - val_accuracy: 0.9333\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 00056: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.3474 - val_accuracy: 0.9333\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9937\n",
      "Epoch 00057: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0285 - accuracy: 0.9937 - val_loss: 0.3070 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.9375\n",
      "Epoch 00058: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.3400 - accuracy: 0.9375 - val_loss: 0.2755 - val_accuracy: 0.9333\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9937\n",
      "Epoch 00059: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0283 - accuracy: 0.9937 - val_loss: 0.3003 - val_accuracy: 0.9333\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9563\n",
      "Epoch 00060: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0949 - accuracy: 0.9563 - val_loss: 0.3176 - val_accuracy: 0.9333\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9875\n",
      "Epoch 00061: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0538 - accuracy: 0.9875 - val_loss: 0.4196 - val_accuracy: 0.9167\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9937\n",
      "Epoch 00062: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0482 - accuracy: 0.9937 - val_loss: 0.3931 - val_accuracy: 0.9333\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9750\n",
      "Epoch 00063: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0892 - accuracy: 0.9750 - val_loss: 0.4806 - val_accuracy: 0.9167\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9750\n",
      "Epoch 00064: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0404 - accuracy: 0.9750 - val_loss: 0.7085 - val_accuracy: 0.8167\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9688\n",
      "Epoch 00065: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1392 - accuracy: 0.9688 - val_loss: 0.3432 - val_accuracy: 0.9333\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9812\n",
      "Epoch 00066: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0633 - accuracy: 0.9812 - val_loss: 0.3973 - val_accuracy: 0.9333\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9875\n",
      "Epoch 00067: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0451 - accuracy: 0.9875 - val_loss: 0.3863 - val_accuracy: 0.9167\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9875\n",
      "Epoch 00068: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0296 - accuracy: 0.9875 - val_loss: 0.3224 - val_accuracy: 0.9333\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9688\n",
      "Epoch 00069: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2032 - accuracy: 0.9688 - val_loss: 0.3364 - val_accuracy: 0.9333\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9750\n",
      "Epoch 00070: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0401 - accuracy: 0.9750 - val_loss: 0.2926 - val_accuracy: 0.9333\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 00071: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.3600 - val_accuracy: 0.9333\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9812\n",
      "Epoch 00072: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0893 - accuracy: 0.9812 - val_loss: 0.4145 - val_accuracy: 0.9167\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9812\n",
      "Epoch 00073: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0570 - accuracy: 0.9812 - val_loss: 0.3949 - val_accuracy: 0.9167\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9688\n",
      "Epoch 00074: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0693 - accuracy: 0.9688 - val_loss: 0.4333 - val_accuracy: 0.9000\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9563\n",
      "Epoch 00075: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.1171 - accuracy: 0.9563 - val_loss: 0.3219 - val_accuracy: 0.9333\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9875\n",
      "Epoch 00076: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0271 - accuracy: 0.9875 - val_loss: 0.3670 - val_accuracy: 0.9000\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9812\n",
      "Epoch 00077: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0625 - accuracy: 0.9812 - val_loss: 0.3010 - val_accuracy: 0.9333\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9812\n",
      "Epoch 00078: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0491 - accuracy: 0.9812 - val_loss: 0.3647 - val_accuracy: 0.9333\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9875\n",
      "Epoch 00079: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0518 - accuracy: 0.9875 - val_loss: 0.3624 - val_accuracy: 0.9500\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2026 - accuracy: 0.9750\n",
      "Epoch 00080: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.2026 - accuracy: 0.9750 - val_loss: 0.3414 - val_accuracy: 0.9333\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3122 - val_accuracy: 0.9333\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9812\n",
      "Epoch 00082: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0438 - accuracy: 0.9812 - val_loss: 0.3646 - val_accuracy: 0.9333\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9875\n",
      "Epoch 00083: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0271 - accuracy: 0.9875 - val_loss: 0.3604 - val_accuracy: 0.9500\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9812\n",
      "Epoch 00084: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0710 - accuracy: 0.9812 - val_loss: 0.3467 - val_accuracy: 0.9167\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9937\n",
      "Epoch 00085: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0224 - accuracy: 0.9937 - val_loss: 0.4721 - val_accuracy: 0.9167\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.9688\n",
      "Epoch 00086: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.1463 - accuracy: 0.9688 - val_loss: 0.2686 - val_accuracy: 0.9333\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9875\n",
      "Epoch 00087: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0391 - accuracy: 0.9875 - val_loss: 0.2624 - val_accuracy: 0.9333\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9750\n",
      "Epoch 00088: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0881 - accuracy: 0.9750 - val_loss: 0.2390 - val_accuracy: 0.9500\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9875\n",
      "Epoch 00089: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0397 - accuracy: 0.9875 - val_loss: 0.5453 - val_accuracy: 0.8667\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9750\n",
      "Epoch 00090: val_accuracy did not improve from 0.95000\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0824 - accuracy: 0.9750 - val_loss: 0.3310 - val_accuracy: 0.9167\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9688"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_set, steps_per_epoch = 10,\n",
    "                                   epochs = 100,\n",
    "                                   callbacks=[checkpoint],\n",
    "                                   validation_data = validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shapes_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:8]] \n",
    "# Extracts the outputs of the top 12 layers\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs) \n",
    "# Creates a model that will return these outputs, given the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "img_path = 'data/triangles/drawing(2).png'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(28, 28))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor /= 255.\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of five Numpy arrays: one array per layer activation\n",
    "activations = activation_model.predict(img_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "for layer in model.layers[:12]:\n",
    "    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n",
    "    \n",
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
    "    n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
    "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,:, :, col * images_per_row + row]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std(ddof=0)\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
