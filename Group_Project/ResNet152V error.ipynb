{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chest X-Ray (Pneumonia): Transfer Learning (b)\n",
    "\n",
    "Using Several estandard models trained on Imagenet \\\n",
    "This example uses a ResNet152V2 network + some additional layers \\\n",
    "We train the new layers and some layers (19) from the original ResNet152V network \n",
    "\n",
    "Train accuracy 96% \\\n",
    "Val accuracy 96% \\\n",
    "Test accuracy 96% \\\n",
    "ROC 99%\n",
    "\n",
    " \\\n",
    "This notebook adds a balancing calculation in both variants \\ \n",
    "We use adaptive learning rate as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T06:50:25.577006Z",
     "iopub.status.busy": "2021-09-21T06:50:25.576634Z",
     "iopub.status.idle": "2021-09-21T06:50:25.603025Z",
     "shell.execute_reply": "2021-09-21T06:50:25.602053Z",
     "shell.execute_reply.started": "2021-09-21T06:50:25.576967Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt    \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "import random\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # to avoid warning messages\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "from keras.applications.resnet import ResNet152\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T06:50:25.605966Z",
     "iopub.status.busy": "2021-09-21T06:50:25.605399Z",
     "iopub.status.idle": "2021-09-21T06:50:25.611930Z",
     "shell.execute_reply": "2021-09-21T06:50:25.611215Z",
     "shell.execute_reply.started": "2021-09-21T06:50:25.605929Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH = 32\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  1583 normal images and 4273 pneumonia images\n"
     ]
    }
   ],
   "source": [
    "# We shuffle the original dataset and create val and test more balanced (0,6 train, 0.2 test, 0.2 val)\n",
    "# folder structure \n",
    "#Notebook\n",
    "#    !------ chest_xray_data\n",
    "#                  !-------------normal\n",
    "#                  !-------------pneumonia\n",
    "\n",
    "train_normal = glob.glob(\"./chest_xray_data/normal/*.jpeg\")\n",
    "train_pneumonia = glob.glob(\"./chest_xray_data/pneumonia/*.jpeg\")\n",
    "print('there are ',len(train_normal),'normal images and', len(train_pneumonia),'pneumonia images')\n",
    "COUNT_PNEUMONIA = len(train_pneumonia)\n",
    "COUNT_NORMAL = len(train_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5856, 2)\n"
     ]
    }
   ],
   "source": [
    "all_list = [x for x in train_normal]\n",
    "all_list.extend([x for x in train_pneumonia])\n",
    "\n",
    "df_all = pd.DataFrame(np.concatenate([['Normal']*len(train_normal) , ['Pneumonia']*len(train_pneumonia)]), \n",
    "                      columns = ['class'])\n",
    "df_all['image'] = [x for x in all_list]\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3923, 2) test (966, 2) val (967, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df, dummy_df = train_test_split(df_all, test_size = 0.33, random_state = SEED, \n",
    "                                    stratify = df_all['class'])\n",
    "test_df, val_df = train_test_split(dummy_df, test_size = 0.50, random_state = SEED, \n",
    "                                    stratify = dummy_df['class'])\n",
    "\n",
    "print('train',train_df.shape, 'test', test_df.shape, 'val', val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial bias: 0.99299\n",
      "Weight for class 0: 1.85\n",
      "Weight for class 1: 0.69\n"
     ]
    }
   ],
   "source": [
    "# to improve class balancing we calculate the weights to add to the training process\n",
    "\n",
    "initial_bias = np.log([COUNT_PNEUMONIA / COUNT_NORMAL])\n",
    "print(\"Initial bias: {:.5f}\".format(initial_bias[0]))\n",
    "\n",
    "TRAIN_IMG_COUNT = COUNT_NORMAL + COUNT_PNEUMONIA\n",
    "weight_for_0 = (1 / COUNT_NORMAL) * (TRAIN_IMG_COUNT) / 2.0\n",
    "weight_for_1 = (1 / COUNT_PNEUMONIA) * (TRAIN_IMG_COUNT) / 2.0\n",
    "\n",
    "calculated_class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(\"Weight for class 0: {:.2f}\".format(weight_for_0))\n",
    "print(\"Weight for class 1: {:.2f}\".format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation is performed in the same way as in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T06:50:31.175831Z",
     "iopub.status.busy": "2021-09-21T06:50:31.172013Z",
     "iopub.status.idle": "2021-09-21T06:50:33.658002Z",
     "shell.execute_reply": "2021-09-21T06:50:33.657320Z",
     "shell.execute_reply.started": "2021-09-21T06:50:31.175748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3923 validated image filenames belonging to 2 classes.\n",
      "Found 967 validated image filenames belonging to 2 classes.\n",
      "Found 966 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255.,\n",
    "                                  zoom_range = 0.1,\n",
    "                                  #rotation_range = 0.1,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "ds_train = train_datagen.flow_from_dataframe(train_df,\n",
    "                                             #directory=train_path, #dataframe contains the full paths\n",
    "                                             x_col = 'image',\n",
    "                                             y_col = 'class',\n",
    "                                             target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                             class_mode = 'binary',\n",
    "                                             batch_size = BATCH,\n",
    "                                             seed = SEED)\n",
    "\n",
    "ds_val = val_datagen.flow_from_dataframe(val_df,\n",
    "                                            #directory=train_path,\n",
    "                                            x_col = 'image',\n",
    "                                            y_col = 'class',\n",
    "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                            class_mode = 'binary',\n",
    "                                            batch_size = BATCH,\n",
    "                                            seed = SEED)\n",
    "\n",
    "ds_test = val_datagen.flow_from_dataframe(test_df,\n",
    "                                            #directory=test_path,\n",
    "                                            x_col = 'image',\n",
    "                                            y_col = 'class',\n",
    "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                            class_mode = 'binary',\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning \n",
    "#### Model 1 - Adding new layers to previously trained network and training only the new ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second approach, called transfer learning, consists of using a pretrained model as a feature extractor. In this notebook, the selected model was the ResNet152V2 available on the Keras Package [(link)](https://keras.io/api/applications/. \n",
    "\n",
    "This model was already trained in another dataset (ImageNet). What we do here is to set include_top to false, removing the ‘head’, responsible for assigning the classes in this other dataset, and keep all the previous layers. Then, we include our last few layers, including the one responsible for generating the output.\n",
    "\n",
    "We train only the new layers, leaving the ResNet152V2 network with the original Imagenet Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting callbakcs\n",
    "\n",
    "initial_learning_rate = 0.015\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=False\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    min_delta=0.0000001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "plateau = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor = 0.2,                                     \n",
    "    patience = 10,                                   \n",
    "    min_delt = 0.0000001,                                \n",
    "    cooldown = 0,                               \n",
    "    verbose = 1\n",
    ") \n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"xray_model.h5\", save_best_only=True)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T08:15:15.707649Z",
     "iopub.status.busy": "2021-09-21T08:15:15.707243Z",
     "iopub.status.idle": "2021-09-21T08:15:15.744581Z",
     "shell.execute_reply": "2021-09-21T08:15:15.743923Z",
     "shell.execute_reply.started": "2021-09-21T08:15:15.707612Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet152V2(      \n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False)\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except for the last 19\n",
    "for layer in base_model.layers[:-19]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainable_network():\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    x = base_model(inputs)\n",
    "\n",
    "    # Head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    #Final Layer (Output)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=[inputs], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T08:15:15.954783Z",
     "iopub.status.busy": "2021-09-21T08:15:15.954569Z",
     "iopub.status.idle": "2021-09-21T08:15:16.008401Z",
     "shell.execute_reply": "2021-09-21T08:15:16.007717Z",
     "shell.execute_reply.started": "2021-09-21T08:15:15.954759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " resnet152v2 (Functional)    (None, 7, 7, 2048)        58331648  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               262272    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,594,049\n",
      "Trainable params: 8,142,081\n",
      "Non-trainable params: 50,451,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Careful if you use adaptive learning rate here, then it hits an error\n",
    "trainable_model = get_trainable_network()\n",
    "trainable_model.compile(loss='binary_crossentropy'\n",
    "                 ,optimizer = keras.optimizers.Adam(learning_rate=lr_schedule), metrics='binary_accuracy')\n",
    "\n",
    "trainable_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-09-21T08:15:16.009843Z",
     "iopub.status.busy": "2021-09-21T08:15:16.009583Z",
     "iopub.status.idle": "2021-09-21T08:51:49.847313Z",
     "shell.execute_reply": "2021-09-21T08:51:49.846579Z",
     "shell.execute_reply.started": "2021-09-21T08:15:16.009803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "123/122 [==============================] - ETA: -3s - loss: 0.3832 - binary_accuracy: 0.8754"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'ExponentialDecay' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26462/204023254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = trainable_model.fit(ds_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculated_class_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplateau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.8/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0mvalue_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seen_so_far\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_base\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'ExponentialDecay' and 'int'"
     ]
    }
   ],
   "source": [
    "history = trainable_model.fit(ds_train,\n",
    "          batch_size = BATCH, epochs = 50,\n",
    "          validation_data=ds_val,\n",
    "          class_weight = calculated_class_weight,\n",
    "          callbacks=[early_stopping, plateau],\n",
    "          steps_per_epoch=(len(train_df)/BATCH),\n",
    "          validation_steps=(len(val_df)/BATCH));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
